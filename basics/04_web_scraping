--------------------------------------------------------------------
CHAPTER 4 - WEB SCRAPING WITH BEAUTIFULSOUP AND SCRAPY
--------------------------------------------------------------------

- Common Web Content Extraction Techniques

    1. Screen Scraping
         Allows you to obtain information by moving around the screen, registering user
           pulsations.

    2. Web Scraping
         Obtain information of a resource (like an HTML page) and extract relevant data.

    3. Report Mining
         Obtain information from a file (HTML, RDF, CSV, ...).

    4. Spiders
         Scripts that follow specific rules to move around the website and gather information
           by imitating user interactions.

    5. Crawlers
         Processes that automatically parse and extract content from a website and provide that
           search content to search engine providers for building their page index.



- HTML Parsers

    - For parsing HTML, the recommended third-party package is lxml, an XML parser.  It's quick, 
        flexible, and tolerant of broken HTML.

        $ pip install lxml


    - Another option is to use BeautifulSoup, which is pure Python.  It can use 'lxml' as a backend
        library.

        $ pip install BeautifulSoup4



- Parsing HTML With lxml

    - The 'lxml' parser is the main module for analysis of XML documents and 'libxslt'.  Its
        main features are:

        1. Support for xml and html
        2. An API based on ElementTree
        3. Support to selected elements using XPath expressions


    - To install lxml:

        $ pip install lxml


    - 'lxml.etree' is a submodule of the lxml libarary that provides methods such as 'XPath()', 
        which supports expressions with XPath selector syntax.  

      In this example, we see the use of the parser to read an HTML file and extract the title
        tag:


        from lxml import html, etree

        simple_page = open('data/simple.html').read()
        parser = etree.HTML(simple_page)
        result = etree.tostring(parser, pretty_print=True, method="html")
        find_text = etree.XPath("//title/text()", smart_strings=False)
        text = find_text(parser)[0]
        print(text)



- Example - Parsing with lxml

    # Download a web page
    >>> import requests
    >>> response = requests.get('https://www.debian.org/releases/stable/index.en.html')

    # Parse the web page into an ElementTree tree
    >>> from lxml.etree import HTML
    >>> root = HTML(response.content)


    # We can use the same syntax as the standard xml library
    >>> [e.tag for e in root]
     ['head', 'body']

    # Get the web page's title
    >>> root.find('head').find('title').text
     'Debian -- Debian “stretch” Release Information '


    # More complex example
    >>> root.find('body').findall('div')[1].find('p').text



- Example - Make a DuckDuckGo Request with lxml

    # duck_duck_go.py

    from lxml.html import fromstring, tostring
    from lxml.html import parse, submit_form
    
    import requests
    response = requests.get('https://duckduckgo.com')
    form_page = fromstring(response.text)
    form = form_page.forms[0]
    print(tostring(form))
    
    page = parse('http://duckduckgo.com').getroot()
    page.forms[0].fields['q'] = 'python'
    result = parse(submit_form(page.forms[0])).getroot()
    print(tostring(result))



- Searching with XPath

    # Get the body element
    >>> root.xpath('body')

    # Get all the div child elements of body
    >>> root.xpath('body/div')


    # Search for all descendents
    >>> root.xpath('//h1')

    # Search for all descendents with an attribute
    >>> root.xpath('//div[@id="content"]')


    # Get all the div elements that have an h1 child
    >>> root.xpath('//div[h1]')

    # Returns element at position in list of matched elements
    >>> root.xpath('body/div[2]')



- Example - Get Debian Version

    # get_debian_version.py

    import re
    import requests
    from lxml.etree import HTML

    response = requests.get('https://www.debian.org/releases/stable/index.en.html')
    root = HTML(response.content)
    
    title_text = root.find('head').find('title').text
    
    if re.search('\u201c(.*)\u201d', title_text):
        release = re.search('\u201c(.*)\u201d', title_text).group(1)
        p_text = root.xpath('//div[@id="content"]/p[1]')[0].text
        version = p_text.split()[1]
        print('Codename: {}\nVersion: {}'.format(release, version))



- Example - Get Links and Images from a URL

    # get_links_images.py

    #!/usr/bin/env python3

    import os
    import requests
    from lxml import html
    
    class Scraping:
        
        def scrapingImages(self,url):
            print("\nGetting images from url:"+ url)
            try:
                response = requests.get(url) 
                parsed_body = html.fromstring(response.text)

                # regular expresion for get images
                images = parsed_body.xpath('//img/@src')
                print('Found images %s' % len(images))

                #create directory for save images
                os.system("mkdir images")
                for image in images:
                    if image.startswith("http") == False:
                        download = url + "/"+ image
                    else:
                        download = image
                    print(download)

                    # download images in images directory
                    r = requests.get(download)
                    f = open('images/%s' % download.split('/')[-1], 'wb')
                    f.write(r.content)
                    f.close()
            except Exception as e:
                print("Connection error in " + url)
                pass

        def scrapingLinks(self,url):
            print("\nGetting links from url:"+ url)
            try:
                response = requests.get(url)
                parsed_body = html.fromstring(response.text)

                # regular expression for get links
                links = parsed_body.xpath('//a/@href')
                print('Found links %s' % len(links))
                for link in links:
                    print(link)
             except Exception as e:
                print("Connection error in " + url)
                pass
     
    if __name__ == "__main__":
        target = "https://news.ycombinator.com"
        scraping = Scraping()
        scraping.scrapingImages(target)
        scraping.scrapingLinks(target)



- BeautifulSoup Introduction

    - BeautifulSoup specializes in searching HTML files by various criteria.  It's main 
        features include:

        1. Parses and allows the extraction of information from HTML documents
        2. Supports multiple parsers in processing XML documents and HTML (lxml, html5lib)
        3. Generates a tree structure with all the elements of the paired documents
        4. Very easily allows the user to search HTML elements


    - To install BeautifulSoup:

        $ pip install BeautifulSoup4


    - To use BeautifulSoup:

        # Pass in HTML document and parser
        >>> from bs4 import BeautifulSoup
        >>> bs = BeautifulSoup(contents, 'lxml')


        # Get all meta tags in a document
        >>> meta_tags = bs.find_all('meta')
        >>> for tag in meta_tags:
                print(tag)


        # Get all forms of a document
        >>> form_tags = bs.find_all('form')
        >>> for form in form_tags:
                print(form)


        # Get all links in a document
        >>> link_tags = bs.find_all('a')
        >>> for link in link_tags:
                print(link)



- Example - Get All Links from a Specified Web Page

    # get_all_links.py

    #!/usr/bin/env python3

    from bs4 import BeautifulSoup
    import requests
    
    url = input("Enter a website to extract the URL's from: ")
    
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get("http://" +url, headers = headers)
    data = response.text
    soup = BeautifulSoup(data, 'lxml')
    for link in soup.find_all('a'):
        print(link.get('href'))



- Example - Download All Images from a Web Page

    # download_images.py

    #!/usr/bin/env python3

    import requests 
    from bs4 import BeautifulSoup 
    import urllib.parse
    import sys
    import os
    
    url = 'http://www.freeimages.co.uk/galleries/transtech/informationtechnology/index.htm'
    response = requests.get(url) 
    parse = BeautifulSoup(response.text, 'lxml')
    
    # Get all image tags
    image_tags = parse.find_all('img')
    
    # Get urls to the images
    images = [url.get('src') for url in image_tags]
    
    # If no images found in the page
    if not images:
        sys.exit("Found No Images")
    
    # Convert relative urls to absolute urls if any
    images = [urllib.parse.urljoin(response.url, url) for url in images] 
    print('Found %s images' % len(images))

    # Create download_images folder if not exists  
    file_path = "download_images"
    directory = os.path.dirname(file_path)
    
    if not os.path.exists(directory):
        try:
            os.makedirs(file_path)
            print ("Creation of the directory %s OK" % file_path)
        except OSError:
            print ("Creation of the directory %s failed" % file_path)
    else:
        print ("download_images directory exists")
     
    # Download images to downloaded folder
    for url in images:response = requests.get(url)
        file = open('download_images/%s' % url.split('/')[-1], 'wb')
        file.write(response.content)
        file.close()
        print('Downloaded %s' % url)



- Example - Get All Links from Hacker News

    # extract_links_hacker_news.py

    #!/usr/bin/env python3
    import requests
    from bs4 import BeautifulSoup
    
    def get_front_page():
        target = "https://news.ycombinator.com"
        frontpage = requests.get(target)
        if not frontpage.ok:
            raise RuntimeError("Can't access hacker news, you should go outside")
        news_soup = BeautifulSoup(frontpage.text,"lxml")
        return news_soup
    
    def find_interesting_links(soup):
        items = soup.findAll('td', {'align': 'right', 'class': 'title'}) 
        links = []
        for i in items:
            try:
                siblings = list(i.next_siblings)
                post_id = siblings[1].find('a')['id']
                link = siblings[2].find('a')['href']
                title = siblings[2].text
                links.append({'link': link, 'title': title,'post_id':post_id})
            except Exception as e:
                pass
        return links
    
    if __name__ == '__main__':
        soup = get_front_page()
        results = find_interesting_links(soup)
        for r in results:
            if r is not None:
                print(r['link'] +" "+(r['title']))



- Extracting Labels Using regex

    - With BeautifulSoup, we can use regex patterns to match specific tags.

        # extract_emails_from_url.py

        import requests
        import re
        from bs4 import BeautifulSoup
        
        url = input("Enter the URL: ")
        response = requests.get(url)
        html_page = response.text
        email_pattern=re.compile(r'\b[\w.-]+?@\w+?\.\w+?\b')
        
        for match in re.findall(email_pattern,html_page):
            print(match)



- Handling URL Exceptions and Not Found Tags

    - Here is an example of error handling for requests:

        # handling_exceptions_tags.py
        from urllib.request import urlopen
        from urllib.error import HTTPError
        from urllib.error import URLError
        from bs4 import BeautifulSoup
         
        try: 
            html = urlopen("https://www.packtpub.com/")
        except HTTPError as e:
            print(e)
        except URLError:
            print("Server down or incorrect domain")
        else:
            res = BeautifulSoup(html.read(),"html5lib")
            if res.title is None:
                print("Tag not found")
            else:
                print(res.title.text)



- Scrapy Introduction

    - Scrapy is an open source platform used to extract data from web pages used for a series
        of applications like data mining and information processing.  It is fast and powerful,
        easily extensible, and portable.

      Scrapy includes the following tools:

        1. Extracting data with CSS selectors and XPath expressions
        2. An interactive console in iPython to extract 
        3. Support for exporting records in JSON, CSV, XML
        4. Support for handling foriegn statements, non-standards, broken code
        5. Extensible, extensions can be created



- Scrapy Architecture

    - Scrapy allows us to recursively scan the contents of a website and apply a set of rules to
        extract useful information.  These are its main elements:

        1. Interpreter
             Allows quick tests and creation of projects with defined structure

        2. Spiders
             Code routines that are responsible for making HTTP requests to a list of domains

        3. XPath Expressions
             Allows us to extract specific information

        4. Items
             Containers of information



- XPath Expressions

    - To use Scrapy, it is necessary to define rules that Scrapy will use for extracting
        information.  These rules can be XPath expressions.  


    - Here is an example:

        # Enter the scrapy shell
        $ scrapy shell

        # Get the page title
        >>> fetch('http://www.scrapy.org')
        >>> response.xpath('//title/text()').extract()
         ['Scrapy | A Fast and Powerful Scraping and Web Crawling Framework']



- Installing Scrapy and Creating a Project

    - Scrapy enables automated and rapid scraping of large amounts of web-based content.  It
        was created from Twisted, so it is capable of performing thousands of queries 
        simultaneously.  It also uses BeautifulSoup and the Python xml package to facilitate
        searches.

        # Install scrapy (depends on having lxml and OpenSSL installed)
        $ pip install scrapy

        # List all scrapy subcommands
        $ scrapy


    - To create a project with Scrapy:

        # Create a new project
        $ scrapy startproject helloProject


        # Here is the directory that was created
        helloProject/
            scrapy.cfg              # Deploy configuration file
            helloProject/           # Project's Python module, you'll import your code from here
                __init__.py
                items.py            # Project items file
                pipelines.py        # Project pipelines file
                settings.py         # Project settings file
                spiders/            # A directory where you'll later put your spiders
                    __init__.py


    - These are the most important files:

        1. items.py
             We define the elements to extract

        2. spiders
             The heart of the project, here we define the extraction procedure

        3. Pipelines.py
             The elements to analyze what has been obtained, data validation and cleaning of HTML



- Scrapy Item Class

    - The 'Item' class is used to define the output data format.  Item objects are containers
        used to collect the extracted data and specify metadata for the fields.

        # MyItem.py

        import scrapy
        from scrapy.loader.processors import TakeFirst

        class MyItem(scrapy.Item):
            # Define fields for items here
            name = scrapy.Field(output_processor=TakeFirst(),)


    - The next step is to describe how the information can be extracted using XPath expressions
        so that Scrapy can differentiate it from the rest of the HTML.

      Here, we start the crawling process:


        # Set up crawler
        >>> from scrapy.crawler import CrawlerProcess
        >>> crawler = CrawlerProcess(settings)

        # Define the spider for the crawler
        >>> crawler.crawl(MySpider())

        # Start Scrapy
        >>> print("STARTING ENGINE")
        >>> crawler.start()

        # Printed at the end of the crawling process
        >>> print("ENGINE STOPPED")


    - We also need to import the necessary modules to carry out the crawling process:

        from scrapy.spiders import CrawlSpider, Rule
        from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor
        from scrapy.selector import HtmlXPathSelector


      Important classes include:

        1. Rule
             Allows us to establish the rules by which the crawler will be based to navigate through
               different links.

        2. LxmlLinkExtractor
             Allows us to define a callback function and regexes to tell the crawler which links to
               go through.  It allows us to define the navigation rules between the links we want
               to obtain.

        3. HtmlXPathSelector
             Allows us to apply XPath expressions.



- Spiders

    - Spiders are classes that define the way to navigate throuth a specific site or domain and how to 
        extract data from those pages.


    - The cycle that follows a spider is the following:

        1. First, we start generating the initial request (Requests) to navigate through the first URL
             and we specify the 'backward' function to be called with the response (Response) downloaded
             from that request.

        2. The first request to be made is obtained by calling the 'start_request()' method, which by 
             default generates the request for the specific URL in the 'start_urls' starting addresses
             and the functions of 'backward' for the requests.



- Creating a Spider

    # spiders/MySpider.py

    from scrapy.contrib.spiders import CrawlSpider, Rule
    from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor
    from scrapy.selector import HtmlXPathSelector
    from scrapy.item import Item
    
    class MySpider(CrawlSpider):
        name = 'example.com'
        allowed_domains = ['example.com']
        start_urls = ['http://www.example.com']
        rules = (Rule(LxmlLinkExtractor(allow=())))
    
        def parse_item(self, response):
            hxs = HtmlXPathSelector(response)
            element = Item()
            return element


    - 'CrawlSpider' provides a mechanism that allows you to follow the links that follow a certain
        pattern.



- Pipelines Items and Export Formats

- Scrapy Settings

- Executing Scrapy

- EuroPython Project

- ScrapingHub

- Portia