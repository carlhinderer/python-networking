--------------------------------------------------------------------
CHAPTER 4 - WEB SCRAPING WITH BEAUTIFULSOUP AND SCRAPY
--------------------------------------------------------------------

- Common Web Content Extraction Techniques

    1. Screen Scraping
         Allows you to obtain information by moving around the screen, registering user
           pulsations.

    2. Web Scraping
         Obtain information of a resource (like an HTML page) and extract relevant data.

    3. Report Mining
         Obtain information from a file (HTML, RDF, CSV, ...).

    4. Spiders
         Scripts that follow specific rules to move around the website and gather information
           by imitating user interactions.

    5. Crawlers
         Processes that automatically parse and extract content from a website and provide that
           search content to search engine providers for building their page index.



- HTML Parsers

    - For parsing HTML, the recommended third-party package is lxml, an XML parser.  It's quick, 
        flexible, and tolerant of broken HTML.

        $ pip install lxml


    - Another option is to use BeautifulSoup, which is pure Python.  It can use 'lxml' as a backend
        library.

        $ pip install BeautifulSoup4



- Parsing HTML With lxml

    - The 'lxml' parser is the main module for analysis of XML documents and 'libxslt'.  Its
        main features are:

        1. Support for xml and html
        2. An API based on ElementTree
        3. Support to selected elements using XPath expressions


    - To install lxml:

        $ pip install lxml


    - 'lxml.etree' is a submodule of the lxml libarary that provides methods such as 'XPath()', 
        which supports expressions with XPath selector syntax.  

      In this example, we see the use of the parser to read an HTML file and extract the title
        tag:


        from lxml import html, etree

        simple_page = open('data/simple.html').read()
        parser = etree.HTML(simple_page)
        result = etree.tostring(parser, pretty_print=True, method="html")
        find_text = etree.XPath("//title/text()", smart_strings=False)
        text = find_text(parser)[0]
        print(text)



- Example - Parsing with lxml

    # Download a web page
    >>> import requests
    >>> response = requests.get('https://www.debian.org/releases/stable/index.en.html')

    # Parse the web page into an ElementTree tree
    >>> from lxml.etree import HTML
    >>> root = HTML(response.content)


    # We can use the same syntax as the standard xml library
    >>> [e.tag for e in root]
     ['head', 'body']

    # Get the web page's title
    >>> root.find('head').find('title').text
     'Debian -- Debian “stretch” Release Information '


    # More complex example
    >>> root.find('body').findall('div')[1].find('p').text



- Example - Make a DuckDuckGo Request with lxml

    # duck_duck_go.py

    from lxml.html import fromstring, tostring
    from lxml.html import parse, submit_form
    
    import requests
    response = requests.get('https://duckduckgo.com')
    form_page = fromstring(response.text)
    form = form_page.forms[0]
    print(tostring(form))
    
    page = parse('http://duckduckgo.com').getroot()
    page.forms[0].fields['q'] = 'python'
    result = parse(submit_form(page.forms[0])).getroot()
    print(tostring(result))



- Searching with XPath

    # Get the body element
    >>> root.xpath('body')

    # Get all the div child elements of body
    >>> root.xpath('body/div')


    # Search for all descendents
    >>> root.xpath('//h1')

    # Search for all descendents with an attribute
    >>> root.xpath('//div[@id="content"]')


    # Get all the div elements that have an h1 child
    >>> root.xpath('//div[h1]')

    # Returns element at position in list of matched elements
    >>> root.xpath('body/div[2]')



- Example - Get Debian Version

    # get_debian_version.py

    import re
    import requests
    from lxml.etree import HTML

    response = requests.get('https://www.debian.org/releases/stable/index.en.html')
    root = HTML(response.content)
    
    title_text = root.find('head').find('title').text
    
    if re.search('\u201c(.*)\u201d', title_text):
        release = re.search('\u201c(.*)\u201d', title_text).group(1)
        p_text = root.xpath('//div[@id="content"]/p[1]')[0].text
        version = p_text.split()[1]
        print('Codename: {}\nVersion: {}'.format(release, version))



- Example - Get Links and Images from a URL

    # get_links_images.py

    #!/usr/bin/env python3

    import os
    import requests
    from lxml import html
    
    class Scraping:
        
        def scrapingImages(self,url):
            print("\nGetting images from url:"+ url)
            try:
                response = requests.get(url) 
                parsed_body = html.fromstring(response.text)

                # regular expresion for get images
                images = parsed_body.xpath('//img/@src')
                print('Found images %s' % len(images))

                #create directory for save images
                os.system("mkdir images")
                for image in images:
                    if image.startswith("http") == False:
                        download = url + "/"+ image
                    else:
                        download = image
                    print(download)

                    # download images in images directory
                    r = requests.get(download)
                    f = open('images/%s' % download.split('/')[-1], 'wb')
                    f.write(r.content)
                    f.close()
            except Exception as e:
                print("Connection error in " + url)
                pass

        def scrapingLinks(self,url):
            print("\nGetting links from url:"+ url)
            try:
                response = requests.get(url)
                parsed_body = html.fromstring(response.text)

                # regular expression for get links
                links = parsed_body.xpath('//a/@href')
                print('Found links %s' % len(links))
                for link in links:
                    print(link)
             except Exception as e:
                print("Connection error in " + url)
                pass
     
    if __name__ == "__main__":
        target = "https://news.ycombinator.com"
        scraping = Scraping()
        scraping.scrapingImages(target)
        scraping.scrapingLinks(target)
