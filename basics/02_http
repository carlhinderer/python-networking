-----------------------------------------------------------
CHAPTER 2 - HTTP
-----------------------------------------------------------

- Consuming Web Services with urllib

    - The 'urllib' module allows access to any resource published on a network through various protocols.
        To start consuming a web service, we have to import these libraries:

        >>> import urllib.request
        >>> import urllib.parse


    - There are 4 functions in urllib:

        1. request = opens and reads the request's URL
        2. error = contains the errors generated by the request
        3. parse = a tool to convert the URL
        4. robotparse = converts the 'robots.txt' file


    - The 'urllib.request' module allows access to a resource published on the internet through its
        address.  The main function that uses this module is 'urlopen'.  

      A 'urlopen' function is used to create an object similar to a file, with which to read from the URL.
        This object has methods like 'read', 'readLine', 'readLines', and 'close', which work exactly the
        same as file methods, although in reality we are using sockets.


    - The 'urlopen' function has an optional data parameter with which to send information to HTTP addresses
        using the 'POST' method.  This parameter is a properly encoded string:

        >>> urllib.request.urlopen (url, data = None, [timeout,] *, 
                                    cafile = None, capath = None, cadefault = False, context = None)



- Retrieving the Contents of a URL

    - Here, we retrieve the contents of a URL:

        >>> from urllib.request import urlopen

        # Send a request and receive a response (in this case an HTML page) from a resource
        >>> response = urlopen('http://packtpub.com')

        # Print the first line of the HTML page we received
        >>> response.readline()


    - The 'urlopen' function also supports specifying a timeout for the request that represents the
        waiting time in the request.  If the request takes longer than we indicated, it will result in 
        an error.  

        >>> print(urlopen('http://packtpub.com', timeout=30))


    - If we receive a JSON response, we can use the 'json' module to parse it.

        >>> import json
        >>> response = urlopen(url, timeout=30)
        >>> json_response = json.loads(response.read())



- HTTP Status Codes

    - We can get the HTTP response code with the 'status' attribute:

        >>> response.status
        200


    - Groups of response codes:

        100 = Informational
        200 = Success
        300 = Redirection
        400 = Client Error
        500 = Server Error



- Handling urllib Exceptions

    - Here, we'll request a nonexistent resource and handle the exception:

        >>> import urllib.error
        >>> from urllib.request import urlopen

        >>> try:
                urlopen('http://www.ietf.org/rfc/rfc0.txt')
            except urllib.error.HTTPError as e:
                print('Exception', e)
                print('status', e.code)
                print('reason', e.reason)
                print('url', e.url)



- HTTP Headers

    - HTTP requests consist of 2 main parts, a header and a body.  Headers are the lines of information
        that contain specific metadata about the response and tell the client how to interpret it.  With
        urllib, we can check the headers.


    - To inspect the headers from a response:

        >>> import urllib.request
        >>> http_response = urllib.request.urlopen(url)

        # Print the header values
        >>> if http_response.code == 200:

                # Print all the headers
                print(http_response.headers)

                # Iterate through the headers
                for key, value in http_response.getheaders():
                    print(key, value)

                # Get just a single header
                user_agent = http_response.getheader('User-agent')



- Customizing urllib Requests

    - To customize requests, we can add headers before sending them.  Here, we retrieve the Dutch version
        of the Debian home page.

        # Create the request
        >>> from urllib.request import Request, urlopen
        >>> req = Request('http://www.debian.org')

        # Add a header to the request
        >>> req.add_header('Accept-Language', 'nl')

        # Submit the customized request
        >>> response = urlopen(req)

        # Print the first 5 lines of the response
        >>> response.readlines()[:5]


    - We can view the headers present in a request before we send it.

        # View the headers in our request
        >>> req.header_items()


    - Here, we add a dictionary of headers to our request:

        # Values of headers for request
        >>> USER_AGENT = 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0'
        >>> URL = 'http://www.debian.org'

        # Create dictionary of headers
        >>> headers = {'Accept-Language': 'nl','User-agent': USER_AGENT}

        # Create the request with out customized values
        >>> request = Request(URL,headers=headers)



- Getting Headers with a Proxy

    - By default, the request handler uses a 'ProxyHandler' instance that reads your environment variables
        to see if a proxy server is specified.  


    - If we need to specify a proxy for our connection, we can use a custom 'ProxyHandler' instance, 
        passing in a dictionary with the address of the proxy server.

        # Parameters for request
        >>> import urllib.request, urllib.parse, urllib.error
        >>> URL = 'https://www.github.com'
        >>> PROXY_ADDRESS = '165.24.10.8:8080'

        # Set up the proxy server
        >>> proxy = urllib.request.ProxyHandler({'http': PROXY_ADDRESS})
        >>> opener = urllib.request.build_opener(proxy)
        >>> urllib.request.install_opener(opener)

        # Build the request using the proxy server
        >>> resp = urllib.request.urlopen(URL)
        >>> print("Proxy server returns response headers: %s" % resp.headers)



- Extracting Links from a URL

    - 'HTMLParser' is a module that allows us to parse text files formatted in HTML.  Here, we request
        a web page and extract all the links in the HTML.

        #!/usr/bin/env python3
        from html.parser import HTMLParser
        import urllib.request

        class myParser(HTMLParser):
            def handle_starttag(self, tag, attrs):
                if (tag == "a"):
                    for a in attrs:
                        if (a[0] == 'href'):
                            link = a[1]
                            if (link.find('http') >= 0):
                                print(link)
                                newParse = myParser()
                                newParse.feed(link)
        
        url = "http://www.packtpub.com"
        request = urllib.request.urlopen(url)
        parser = myParser()
        parser.feed(request.read().decode('utf-8'))


    - Alternatively, we can extract the links using regular expressions.

        #!/usr/bin/env python3

        from urllib.request import urlopen
        import re
        
        def download_page(url):
            return urlopen(url).read().decode('utf-8')
        
        def extract_links(page):
            link_regex = re.compile('<a[^>]+href=["\'](.*?)["\']',re.IGNORECASE)
            return link_regex.findall(page)
        
        if __name__ == '__main__':
            target_url = 'http://www.packtpub.com'
            packtpub = download_page(target_url)
            links = extract_links(packtpub)
            for link in links:
                print(link)



- Getting Images from a URL with urllib

    - Similarly to how we extracted links with regular expressions, we can extract image locations by
        looking for <img> tags.

        >>> def extract_image_locations(page):
                img_regex = re.compile('<img[^>]+src=["\'](.*?)["\']', re.IGNORECASE)
                return img_regex.findall(page)



- Working with URLs

    - The 'urllib.parse' module is used for breaking a URL into its component parts.  

        >>> from ulrllib.parse import urlparse

        # Parse a URL and look at its components
        >>> result = urlparse('https://www.packtpub.com/tech/Python')

        >>> result
         ParseResult(scheme='http', netloc='www.packtpub.com', path='/tech/Python',
                     params='', query='', fragment='')


    - Here is a url with a query string:

        >>> result = urlparse('https://search.packtpub.com/?query=python')

        >>> result.query
        'query=python'


    - Here, we convert the query string parameters into a dictionary:

        >>> from urllib.parse import parse_qs
        >>> result = result = urlparse('https://search.packtpub.com/?query=python')

        >>> parse_qs(result.query)
        {'query': ['python']}


    - Here, we encode a dictionary as a query string:

        >>> from urllib.parse import urlencode
        >>> params = urllib.parse.urlencode({'user': 'user123', 'password': 'pw123'})

        >>> params
         'user=user123&password=pw123'



- The 'requests' Module

    - The 'requests' model has much simplified syntax for making HTTP requests.

        >>> import requests
        >>> response = requests.get('http://www.github.com')


    - The response object has these fields:

        'status_code' = the HTTP code returned by the server
        'content'     = the content of the server response
        'json'        = if the answer is json, this method serializes the string and returns a dictionary


    - Here, we look at a few properties of the response:

        >>> response.status_code
        200

        >>> response.reason
        'OK'

        >>> response.url
        'http://www.github.com/'

        >>> response.text
        Returns entire response, similar to 'content'


    - To inspect the headers in the response:

        # Get all the headers in dictionary
        >>> headers = response.headers

        # Get a single header
        >>> respones.headers['content-type']
        'text/html; charset=utf-8'



- Using Proxy Servers with requests

    >>> import requests

    # Configure proxy settings
    >>> http_proxy = 'http://<ip_address>:<port>'
    >>> proxy_dictionary = {'http': http_proxy}

    # Make a request using the proxy settings
    >>> requests.get('http://example.org', proxies=proxy_dictionary)



- Getting whois Information

    - We can use the requests module and the 'whois.domaintools.com' service to get information about the
        domain we are analyzing, such as the IP address and location.

        >>> domain = input("Enter the domain : ")
        >>> url = 'http://whois.domaintools.com/' + domain
        >>> headers = {'User-Agent': 'wswp'}
        >>> resp = requests.get(url, headers=headers)



- Working with JSON

    - If we need to send JSON from a client to a server, the simplest way is to use the 'json' parameter,
        specifying a dictionary structure in key/value format.

      The main advantage of this approach is that we don't have to specify the 'Content-Type' in the request.


       >>> import requests
       >>> response = requests.post('http://httpbin.org/post', json={'key': 'value'})

       >>> response.status_code
       200

       >>> response.json()
        {'args': {},
         'data': '{"key": "value"}',
         'files': {},
         'form': {},
         'headers': {'Accept': '*/*',
         'Accept-Encoding': 'gzip, deflate',
         'Connection': 'close',
         'Content-Length': '16',
         'Content-Type': 'application/json',
         'Host': 'httpbin.org',
         'User-Agent': 'python-requests/2.4.3 CPython/3.4.0',
         'X-Request-Id': 'xx-xx-xx'},
         'json': {'key': 'value'},
         'origin': 'x.x.x.x',
         'url': 'http://httpbin.org/post'}



- Handling Forms with urllib

    - Forms use the POST method to send data to the server.  The data entered into the form will go 
        into the body of the request.  We can put any bytes data in there and declare its type by adding
        a 'Content-Type' header to our request with the appropriate MIME type.


    - Suppose we have a form where a customer enters their name, phone number, email address, and desired
        pizza size.  The form information will be passed through a dictionary structure in an extra 
        field called data.  The form values must be formatted the same way as query string parameters,
        and must be URL-encoded.  Also, 'Content-Type' must be set to a MIME type of
        'applcation/x-www-form-urlencoded'.


    - To do this in urllib:

        # Specify form entries
        >>> data_dictionary = {'custname': 'customer','custtel': '323232', 'size': 'large',
                               'custemail': 'email@domain.com'}

        # Encode the data
        >>> data = urlencode(data_dictionary).encode('utf-8')

        # Add the data to the request, which automatically makes it a POST request
        >>> from urllib import Request
        >>> req = Request('http://httpbin.org/post', data=data)

        # Set the MIME type in the header
        >>> req.add_header('Content-Type', 'application/x-www-form-urlencode;charset=UTF-8')

        # Send the request
        >>> response = urlopen(req)

        # Look at the response
        >>> response_dictionary = json.load(response)
        >>> print(response_dictionary)
        {'args': {}, 
         'data': 'custname=customer&custtel=323232&size=large&custemail=email%40domain.com', 
         'files': {}, 
         'form': {}, 
         'headers': {'Accept-Encoding': 'identity', 'Connection': 'close', 'Content-Length': '72', 
                     'Content-Type': 'application/x-www-form-urlencode;charset=UTF-8', 
                     'Host': 'httpbin.org', 'User-Agent': 'Python-urllib/3.6'},
         ...
        }



- Handling Forms with requests

    - The requests library takes care of the encoding and formatting for us.

        >>> import requests
        >>> data_dictionary = {'custname': 'customer','custtel': '323232', 'size': 'large',
                               'custemail': 'email@domain.com'}

        # Send request
        >>> response = requests.post("http://httpbin.org/post",data=data_dictionary)

        # Print out the http status_code
        >>> print("HTTP Status Code: " + str(response.status_code))



- Cookies

    - A cookie is a file created by a website that contains small amounts of data and is sent between a
        sender and a receiver.  A cookie's main purpose is to identify the user by storing their activity
        history on a specific website, so the most appropriate content according to their habits can be
        offered.


    - Each time a website is visited for the first time, a cookie is saved in the browser with little
        information.  Then, when the same page is visited again, the server asks for the same cookie to
        fix the configuration of the site and make the visit personalized.  They may have a simple purpose,
        like recording the last time the user visited a web site, or a more complex purpose, like storing
        a shopping cart full of items.


    - 'Session Cookies' have a short lifespan, since they are deleted when the browser is closed.  
        'Persistent Cookies' are deleted only by clearing browser data, and may also have an expiration
        data attached.


    - Secure cookies store encrypted information to prevent the data stored in them from being vulnerable
        to 3rd party attacks.  They are used only in HTTPS connections.


    - Servers use cookies in various ways.  They can add a unique ID, which tracks a client as they move
        around the site.  They can store a login token, which will automatically log the client back in
        if they leave the site and come back.  They can be used for storing user preferences.


    - Because HTTP is a stateless protocol, cookies are the only way of tracking a client between requests.



- Handling Cookies with urllib

    - To work with cookies in urllib, we can use the 'HTTPCookieProcessor' handler from 'urllib.request'.

        >>> import urllib
        >>> cookie_processor = urllib.request.HTTPCookieProcessor()


    - If we want to access these cookies or send our own cookies, we can pass a 'CookieJar' object to
        the initializer.  This will automatically extract the cookies that a server sends us.

        # Create an empty cookie jar
        >>> from http.cookiejar import CookieJar
        >>> cookie_jar = CookieJar()

        # Create and install an opener
        >>> cookie_processor = urllib.request.HTTPCookieProcessor(cookie_jar)
        >>> opener = urllib.request.build_opener(cookie_processor)
        >>> urllib.install_opener(opener)

        # Use the opener to make an http request
        >>> opener.open('http://www.github.com')
        >>> len(cookie_jar)
        3


    - Whenever we use the opener to make further requests, the 'HTTPCookieProcessor' will check our
        'cookie_jar' to see if it contains any cookies for the site, and will automatically add them to
        our requests.  It will also add any further cookies received to the jar.



- Inspecting Cookies with urllib

    - To inspect the cookies we received:

        >>> cookies = list(cookie_jar)
        >>> cookies
        [Cookie(version=0, name='logged_in', value='no', port=None, port_specified=False,
                domain='.github.com', domain_specified=True, domain_initial_dot=True, path='/',
                path_specified=True, secure=True, expires=2173978199, discard=False, comment=None,
                comment_url=None, rest={'HttpOnly': None}, rfc2109=False), 

        Cookie(version=0, name='_gh_sess', 
               value='cDlMVjFOdHM1djhLYWRGeGpPMTIxLytnRzdHNlRQT3VkUngxLzY2UkpmcGI1KzhNYWd4TzBzb2VJK0cxcWZ
               BeGdpOFA3ZE95RXd5Nnp0WDdDWlQ0dHpwSkYzQ0hOZ2o1R3JkOXBPZTdCL0N4dGVMZ0dHc0VKZ0RreW5raDdRNDcrS
               0tTTlZiY1pOcGw5NkdkNDZBTnlQNTBiTDRzRTRIeVZPNVY2RWdUZ3VvYkFsczNqd3psQ0JBSld2Rlk4d3QvQm5XRm1
               iSGtVeVpTdG9haVVzMFhnQT09LS1pR3NNNnN1VmRocVJ0RXpkaWhxK0JRPT0%3D--84304a9b84b7e8c2605efb980
               8c1b92a25fcc221', 
               port=None, port_specified=False, domain='github.com', domain_specified=False,
               domain_initial_dot=False, path='/', path_specified=True, secure=True, expires=None,
               discard=True, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False), 

        Cookie(version=0, name='has_recent_activity', value='1', port=None, port_specified=False,
               domain='github.com', domain_specified=False, domain_initial_dot=False, path='/',
               path_specified=True, secure=False, expires=1542829799, discard=False, comment=None,
               comment_url=None, rest={}, rfc2109=False)
        ]


    - 