-----------------------------------------------------------
CHAPTER 2 - HTTP
-----------------------------------------------------------

- Consuming Web Services with urllib

    - The 'urllib' module allows access to any resource published on a network through various protocols.
        To start consuming a web service, we have to import these libraries:

        >>> import urllib.request
        >>> import urllib.parse


    - There are 4 functions in urllib:

        1. request = opens and reads the request's URL
        2. error = contains the errors generated by the request
        3. parse = a tool to convert the URL
        4. robotparse = converts the 'robots.txt' file


    - The 'urllib.request' module allows access to a resource published on the internet through its
        address.  The main function that uses this module is 'urlopen'.  

      A 'urlopen' function is used to create an object similar to a file, with which to read from the URL.
        This object has methods like 'read', 'readLine', 'readLines', and 'close', which work exactly the
        same as file methods, although in reality we are using sockets.


    - The 'urlopen' function has an optional data parameter with which to send information to HTTP addresses
        using the 'POST' method.  This parameter is a properly encoded string:

        >>> urllib.request.urlopen (url, data = None, [timeout,] *, 
                                    cafile = None, capath = None, cadefault = False, context = None)



- Retrieving the Contents of a URL

    - Here, we retrieve the contents of a URL:

        >>> from urllib.request import urlopen

        # Send a request and receive a response (in this case an HTML page) from a resource
        >>> response = urlopen('http://packtpub.com')

        # Print the first line of the HTML page we received
        >>> response.readline()


    - The 'urlopen' function also supports specifying a timeout for the request that represents the
        waiting time in the request.  If the request takes longer than we indicated, it will result in 
        an error.  

        >>> print(urlopen('http://packtpub.com', timeout=30))


    - If we receive a JSON response, we can use the 'json' module to parse it.

        >>> import json
        >>> response = urlopen(url, timeout=30)
        >>> json_response = json.loads(response.read())



- HTTP Status Codes

    - We can get the HTTP response code with the 'status' attribute:

        >>> response.status
        200


    - Groups of response codes:

        100 = Informational
        200 = Success
        300 = Redirection
        400 = Client Error
        500 = Server Error



- Handling urllib Exceptions

    - Here, we'll request a nonexistent resource and handle the exception:

        >>> import urllib.error
        >>> from urllib.request import urlopen

        >>> try:
                urlopen('http://www.ietf.org/rfc/rfc0.txt')
            except urllib.error.HTTPError as e:
                print('Exception', e)
                print('status', e.code)
                print('reason', e.reason)
                print('url', e.url)



- HTTP Headers

    - HTTP requests consist of 2 main parts, a header and a body.  Headers are the lines of information
        that contain specific metadata about the response and tell the client how to interpret it.  With
        urllib, we can check the headers.


    - To inspect the headers from a response:

        >>> import urllib.request
        >>> http_response = urllib.request.urlopen(url)\

        # Print the header values
        >>> if http_response.code == 200:

                # Print all the headers
                print(http_response.headers)

                # Iterate through the headers
                for key, value in http_response.getheaders():
                    print(key, value)

                # Get just a single header
                user_agent = http_response.getheader('User-agent')



- Customizing urllib Requests

    - To customize requests, we can add headers before sending them.  Here, we retrieve the Dutch version
        of the Debian home page.

        # Create the request
        >>> from urllib.request import Request, urlopen
        >>> req = Request('http://www.debian.org')

        # Add a header to the request
        >>> req.add_header('Accept-Language', 'nl')

        # Submit the customized request
        >>> response = urlopen(req)

        # Print the first 5 lines of the response
        >>> response.readlines()[:5]


    - We can view the headers present in a request before we send it.

        # View the headers in our request
        >>> req.header_items()


    - Here, we add a dictionary of headers to our request:

        # Values of headers for request
        >>> USER_AGENT = 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0'
        >>> URL = 'http://www.debian.org'

        # Create dictionary of headers
        >>> headers = {'Accept-Language': 'nl','User-agent': USER_AGENT}

        # Create the request with out customized values
        >>> request = Request(URL,headers=headers)



- Getting Headers with a Proxy

    - By default, the request handler uses a 'ProxyHandler' instance that reads your environment variables
        to see if a proxy server is specified.  


    - If we need to specify a proxy for our connection, we can use a custom 'ProxyHandler' instance, 
        passing in a dictionary with the address of the proxy server.

        # Parameters for request
        >>> import urllib.request, urllib.parse, urllib.error
        >>> URL = 'https://www.github.com'
        >>> PROXY_ADDRESS = '165.24.10.8:8080'

        # Set up the proxy server
        >>> proxy = urllib.request.ProxyHandler({'http': PROXY_ADDRESS})
        >>> opener = urllib.request.build_opener(proxy)
        >>> urllib.request.instalL_opener(opener)

        # Build the request using the proxy server
        >>> resp = urllib.request.urlopen(URL)
        >>> print("Proxy server returns response headers: %s" % resp.headers)



- Extracting Links from a URL

    - 'HTMLParser' is a module that allows us to parse text files formatted in HTML.  Here, we request
        a web page and extract all the links in the HTML.

        #!/usr/bin/env python3
        from html.parser import HTMLParser
        import urllib.request

        class myParser(HTMLParser):
            def handle_starttag(self, tag, attrs):
                if (tag == "a"):
                    for a in attrs:
                        if (a[0] == 'href'):
                            link = a[1]
                            if (link.find('http') >= 0):
                                print(link)
                                newParse = myParser()
                                newParse.feed(link)
        
        url = "http://www.packtpub.com"
        request = urllib.request.urlopen(url)
        parser = myParser()
        parser.feed(request.read().decode('utf-8'))