-----------------------------------------------------------
CHAPTER 2 - HTTP
-----------------------------------------------------------

- Consuming Web Services with urllib

    - The 'urllib' module allows access to any resource published on a network through various protocols.
        To start consuming a web service, we have to import these libraries:

        >>> import urllib.request
        >>> import urllib.parse


    - There are 4 functions in urllib:

        1. request = opens and reads the request's URL
        2. error = contains the errors generated by the request
        3. parse = a tool to convert the URL
        4. robotparse = converts the 'robots.txt' file


    - The 'urllib.request' module allows access to a resource published on the internet through its
        address.  The main function that uses this module is 'urlopen'.  

      A 'urlopen' function is used to create an object similar to a file, with which to read from the URL.
        This object has methods like 'read', 'readLine', 'readLines', and 'close', which work exactly the
        same as file methods, although in reality we are using sockets.


    - The 'urlopen' function has an optional data parameter with which to send information to HTTP addresses
        using the 'POST' method.  This parameter is a properly encoded string:

        >>> urllib.request.urlopen (url, data = None, [timeout,] *, 
                                    cafile = None, capath = None, cadefault = False, context = None)



- Retrieving the Contents of a URL

    - Here, we retrieve the contents of a URL:

        >>> from urllib.request import urlopen

        # Send a request and receive a response (in this case an HTML page) from a resource
        >>> response = urlopen('http://packtpub.com')

        # Print the first line of the HTML page we received
        >>> response.readline()


    - The 'urlopen' function also supports specifying a timeout for the request that represents the
        waiting time in the request.  If the request takes longer than we indicated, it will result in 
        an error.  

        >>> print(urlopen('http://packtpub.com', timeout=30))


    - If we receive a JSON response, we can use the 'json' module to parse it.

        >>> import json
        >>> response = urlopen(url, timeout=30)
        >>> json_response = json.loads(response.read())



- HTTP Status Codes

    - We can get the HTTP response code with the 'status' attribute:

        >>> response.status
        200


    - Groups of response codes:

        100 = Informational
        200 = Success
        300 = Redirection
        400 = Client Error
        500 = Server Error



- Handling urllib Exceptions

    - Here, we'll request a nonexistent resource and handle the exception:

        >>> import urllib.error
        >>> from urllib.request import urlopen

        >>> try:
                urlopen('http://www.ietf.org/rfc/rfc0.txt')
            except urllib.error.HTTPError as e:
                print('Exception', e)
                print('status', e.code)
                print('reason', e.reason)
                print('url', e.url)



- HTTP Headers

    - HTTP requests consist of 2 main parts, a header and a body.  Headers are the lines of information
        that contain specific metadata about the response and tell the client how to interpret it.  With
        urllib, we can check the headers.


    - To inspect the headers from a response:

        >>> import urllib.request
        >>> http_response = urllib.request.urlopen(url)

        # Print the header values
        >>> if http_response.code == 200:

                # Print all the headers
                print(http_response.headers)

                # Iterate through the headers
                for key, value in http_response.getheaders():
                    print(key, value)

                # Get just a single header
                user_agent = http_response.getheader('User-agent')



- Customizing urllib Requests

    - To customize requests, we can add headers before sending them.  Here, we retrieve the Dutch version
        of the Debian home page.

        # Create the request
        >>> from urllib.request import Request, urlopen
        >>> req = Request('http://www.debian.org')

        # Add a header to the request
        >>> req.add_header('Accept-Language', 'nl')

        # Submit the customized request
        >>> response = urlopen(req)

        # Print the first 5 lines of the response
        >>> response.readlines()[:5]


    - We can view the headers present in a request before we send it.

        # View the headers in our request
        >>> req.header_items()


    - Here, we add a dictionary of headers to our request:

        # Values of headers for request
        >>> USER_AGENT = 'Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0'
        >>> URL = 'http://www.debian.org'

        # Create dictionary of headers
        >>> headers = {'Accept-Language': 'nl','User-agent': USER_AGENT}

        # Create the request with out customized values
        >>> request = Request(URL,headers=headers)



- Getting Headers with a Proxy

    - By default, the request handler uses a 'ProxyHandler' instance that reads your environment variables
        to see if a proxy server is specified.  


    - If we need to specify a proxy for our connection, we can use a custom 'ProxyHandler' instance, 
        passing in a dictionary with the address of the proxy server.

        # Parameters for request
        >>> import urllib.request, urllib.parse, urllib.error
        >>> URL = 'https://www.github.com'
        >>> PROXY_ADDRESS = '165.24.10.8:8080'

        # Set up the proxy server
        >>> proxy = urllib.request.ProxyHandler({'http': PROXY_ADDRESS})
        >>> opener = urllib.request.build_opener(proxy)
        >>> urllib.request.install_opener(opener)

        # Build the request using the proxy server
        >>> resp = urllib.request.urlopen(URL)
        >>> print("Proxy server returns response headers: %s" % resp.headers)



- Extracting Links from a URL

    - 'HTMLParser' is a module that allows us to parse text files formatted in HTML.  Here, we request
        a web page and extract all the links in the HTML.

        #!/usr/bin/env python3
        from html.parser import HTMLParser
        import urllib.request

        class myParser(HTMLParser):
            def handle_starttag(self, tag, attrs):
                if (tag == "a"):
                    for a in attrs:
                        if (a[0] == 'href'):
                            link = a[1]
                            if (link.find('http') >= 0):
                                print(link)
                                newParse = myParser()
                                newParse.feed(link)
        
        url = "http://www.packtpub.com"
        request = urllib.request.urlopen(url)
        parser = myParser()
        parser.feed(request.read().decode('utf-8'))


    - Alternatively, we can extract the links using regular expressions.

        #!/usr/bin/env python3

        from urllib.request import urlopen
        import re
        
        def download_page(url):
            return urlopen(url).read().decode('utf-8')
        
        def extract_links(page):
            link_regex = re.compile('<a[^>]+href=["\'](.*?)["\']',re.IGNORECASE)
            return link_regex.findall(page)
        
        if __name__ == '__main__':
            target_url = 'http://www.packtpub.com'
            packtpub = download_page(target_url)
            links = extract_links(packtpub)
            for link in links:
                print(link)



- Getting Images from a URL with urllib

    - Similarly to how we extracted links with regular expressions, we can extract image locations by
        looking for <img> tags.

        >>> def extract_image_locations(page):
                img_regex = re.compile('<img[^>]+src=["\'](.*?)["\']', re.IGNORECASE)
                return img_regex.findall(page)



- Working with URLs

    - The 'urllib.parse' module is used for breaking a URL into its component parts.  

        >>> from ulrllib.parse import urlparse

        # Parse a URL and look at its components
        >>> result = urlparse('https://www.packtpub.com/tech/Python')

        >>> result
         ParseResult(scheme='http', netloc='www.packtpub.com', path='/tech/Python',
                     params='', query='', fragment='')


    - Here is a url with a query string:

        >>> result = urlparse('https://search.packtpub.com/?query=python')

        >>> result.query
        'query=python'


    - Here, we convert the query string parameters into a dictionary:

        >>> from urllib.parse import parse_qs
        >>> result = result = urlparse('https://search.packtpub.com/?query=python')

        >>> parse_qs(result.query)
        {'query': ['python']}


    - Here, we encode a dictionary as a query string:

        >>> from urllib.parse import urlencode
        >>> params = urllib.parse.urlencode({'user': 'user123', 'password': 'pw123'})

        >>> params
         'user=user123&password=pw123'



- The 'requests' Module

    - The 'requests' model has much simplified syntax for making HTTP requests.

        >>> import requests
        >>> response = requests.get('http://www.github.com')


    - The response object has these fields:

        'status_code' = the HTTP code returned by the server
        'content'     = the content of the server response
        'json'        = if the answer is json, this method serializes the string and returns a dictionary


    - Here, we look at a few properties of the response:

        >>> response.status_code
        200

        >>> response.reason
        'OK'

        >>> response.url
        'http://www.github.com/'

        >>> response.text
        Returns entire response, similar to 'content'


    - To inspect the headers in the response:

        # Get all the headers in dictionary
        >>> headers = response.headers

        # Get a single header
        >>> respones.headers['content-type']
        'text/html; charset=utf-8'



- Using Proxy Servers with requests

    >>> import requests

    # Configure proxy settings
    >>> http_proxy = 'http://<ip_address>:<port>'
    >>> proxy_dictionary = {'http': http_proxy}

    # Make a request using the proxy settings
    >>> requests.get('http://example.org', proxies=proxy_dictionary)



- Getting whois Information

    - We can use the requests module and the 'whois.domaintools.com' service to get information about the
        domain we are analyzing, such as the IP address and location.

        >>> domain = input("Enter the domain : ")
        >>> url = 'http://whois.domaintools.com/' + domain
        >>> headers = {'User-Agent': 'wswp'}
        >>> resp = requests.get(url, headers=headers)



- Working with JSON

    - If we need to send JSON from a client to a server, the simplest way is to use the 'json' parameter,
        specifying a dictionary structure in key/value format.

      The main advantage of this approach is that we don't have to specify the 'Content-Type' in the request.


       >>> import requests
       >>> response = requests.post('http://httpbin.org/post', json={'key': 'value'})

       >>> response.status_code
       200

       >>> response.json()
        {'args': {},
         'data': '{"key": "value"}',
         'files': {},
         'form': {},
         'headers': {'Accept': '*/*',
         'Accept-Encoding': 'gzip, deflate',
         'Connection': 'close',
         'Content-Length': '16',
         'Content-Type': 'application/json',
         'Host': 'httpbin.org',
         'User-Agent': 'python-requests/2.4.3 CPython/3.4.0',
         'X-Request-Id': 'xx-xx-xx'},
         'json': {'key': 'value'},
         'origin': 'x.x.x.x',
         'url': 'http://httpbin.org/post'}



- Handling Forms with urllib

    - Forms use the POST method to send data to the server.  The data entered into the form will go 
        into the body of the request.  We can put any bytes data in there and declare its type by adding
        a 'Content-Type' header to our request with the appropriate MIME type.


    - Suppose we have a form where a customer enters their name, phone number, email address, and desired
        pizza size.  The form information will be passed through a dictionary structure in an extra 
        field called data.  The form values must be formatted the same way as query string parameters,
        and must be URL-encoded.  Also, 'Content-Type' must be set to a MIME type of
        'applcation/x-www-form-urlencoded'.


    - To do this in urllib:

        # Specify form entries
        >>> data_dictionary = {'custname': 'customer','custtel': '323232', 'size': 'large',
                               'custemail': 'email@domain.com'}

        # Encode the data
        >>> data = urlencode(data_dictionary).encode('utf-8')

        # Add the data to the request, which automatically makes it a POST request
        >>> from urllib import Request
        >>> req = Request('http://httpbin.org/post', data=data)

        # Set the MIME type in the header
        >>> req.add_header('Content-Type', 'application/x-www-form-urlencode;charset=UTF-8')

        # Send the request
        >>> response = urlopen(req)

        # Look at the response
        >>> response_dictionary = json.load(response)
        >>> print(response_dictionary)
        {'args': {}, 
         'data': 'custname=customer&custtel=323232&size=large&custemail=email%40domain.com', 
         'files': {}, 
         'form': {}, 
         'headers': {'Accept-Encoding': 'identity', 'Connection': 'close', 'Content-Length': '72', 
                     'Content-Type': 'application/x-www-form-urlencode;charset=UTF-8', 
                     'Host': 'httpbin.org', 'User-Agent': 'Python-urllib/3.6'},
         ...
        }